















































import math
import os
import sys
import warnings
from datetime import datetime
from pathlib import Path

import itables
import itables.options as opt
import joblib
import numpy as np
import pandas as pd
import yaml
from ydata_profiling import ProfileReport

# jupyer notebook libraries
opt.columnDefs = [
    {"className": "dt-center", "targets": "_all"}
]  # Center-align all columns
opt.maxBytes = 0  # Display full content, no truncation
# Configure itables to display DataFrames automatically
itables.init_notebook_mode(all_interactive=True)


# Fix the import statements
sys.path = list(dict.fromkeys(sys.path))
print(sys.path)
print("Before CWD: ", os.getcwd())


# Your desired path
PROJET_DIR = os.path.abspath("/Users/tkax/dev/aimonetize/WIP/LeoVegasChurn")
os.chdir(PROJET_DIR)
print("Current CWD: ", os.getcwd())
# Suppress FutureWarnings for cleaner output
warnings.filterwarnings("ignore", category=FutureWarning)

# Check if the path is already in sys.path and add it if it's not
if PROJET_DIR not in sys.path:
    sys.path.append(PROJET_DIR)


from src.config.load_config import load_config, load_config_files
from src.ETL.filesystem.read_csv import read_csv

# def load_config(config_path="config.yaml"):
#     with open(config_path, "r") as file:
#         return yaml.safe_load(file)


# def load_config_files(
#     path="/Users/tkax/dev/aimonetize/WIP/LeoVegasChurn/configs/config.yaml",
# ):
#     """
#     Load configuration files from the specified path.

#     Args:
#         - path (str): The path to the configuration files.

#     Returns:
#         - dict: The configuration dictionary.
#     """
#     # config_file_path = "/Users/tkax/dev/aimonetize/WIP/ProjectTemplates/predictAPI/sales-forecasting/configs/config.yaml"
#     config = load_config(path)
#     model_type = config["info"]["model_type"]
#     model_config_folder = config["info"]["model_config_folder"]
#     model_config = load_config(os.path.join(model_config_folder, f"{model_type}.yaml"))
#     return config, model_config


config, model_config = load_config_files(
    path=os.path.join(PROJET_DIR, "configs/config.yaml")
)
print(config)
get_ipython().run_line_magic("matplotlib", " inline")





def ETL(config):
    """
    Extract, transform, and load (ETL) the data.

    Args:
        - config (dict): The configuration dictionary containing the ETL paths.

    Returns:
        - data (pandas DataFrame): The loaded and processed data.
    """
    raw_data_path = config["etl"]["raw_data_path"]
    processed_data_path = config["etl"]["processed_data_path"]

    data = read_csv(raw_data_path)
    # data = data.head(1000)  # Example: limit to 1000 rows for testing
    path_processed_data_path = Path(processed_data_path)

    # Check if the file exists before saving
    path_processed_data_path.parent.mkdir(parents=True, exist_ok=True)

    # Load (save) only if file does not exist
    if not path_processed_data_path.exists():
        joblib.dump(data, processed_data_path)
        print(f"Data saved to {processed_data_path}")
    else:
        print(f"File already exists at {processed_data_path}. No new data saved.")

    print(f"Columns: {data.columns}")
    print(f"DTypes: {data.dtypes}")
    print(f"Samle data: {data.head}")
    print(f"Shape of df: {data.shape}")
    return data


df = ETL(config)


# Fix the  column dtypes:
# - integers -> floats
# - string dates -> datetimes


def convert_columns_to_datetime(df, columns):
    """
    Converts specified columns in a DataFrame from string to datetime format automatically.

    Args:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of column names to be converted.

    Returns:
        pd.DataFrame: DataFrame with specified columns converted to datetime where applicable.
    """
    for col in columns:
        try:
            # Attempt conversion without infer_datetime_format
            converted_col = pd.to_datetime(df[col], errors="coerce")

            # Check if conversion was successful (less than 50% NaT after conversion)
            if converted_col.notna().sum() / len(df) > 0.5:
                df[col] = converted_col
                print(f"Column '{col}' successfully converted to datetime.")
            else:
                print(
                    f"Column '{col}' could not be reliably converted to datetime. Skipping."
                )

        except Exception as e:
            print(f"Error converting column '{col}': {e}")

    return df


def convert_birth_year_to_datetime(df, column_name="birth_year"):
    """
    Converts a column representing birth years (as integers) to datetime.

    Args:
        df (pd.DataFrame): The input DataFrame.
        column_name (str): The name of the column containing birth years.

    Returns:
        pd.DataFrame: DataFrame with the birth year column converted to datetime.
    """
    if column_name in df.columns:
        try:
            # Convert the integer year to a datetime, defaulting to January 1st of that year
            df[column_name] = pd.to_datetime(
                df[column_name], format="%Y", errors="coerce"
            )
            print(f"Column '{column_name}' successfully converted to datetime.")
        except Exception as e:
            print(f"Error converting column '{column_name}': {e}")
    else:
        print(f"Column '{column_name}' not found in DataFrame.")

    return df


def calculate_age(df, birth_date_column="birth_year", age_column="age"):
    """
    Calculates age from a datetime birth year column and adds it to the DataFrame.

    Args:
        df (pd.DataFrame): The input DataFrame.
        birth_date_column (str): The name of the column with birth dates (in datetime format).
        age_column (str): The name of the column to store the calculated age.

    Returns:
        pd.DataFrame: DataFrame with an additional 'age' column.
    """
    if birth_date_column in df.columns:
        try:
            # Get the current date
            today = pd.Timestamp(datetime.today().date())

            # Calculate the age
            df[age_column] = df[birth_date_column].apply(
                lambda birth_date: math.ceil(
                    (today.year - birth_date.year)
                    - ((today.month, today.day) < (birth_date.month, birth_date.day))
                )
            )

            print(f"Age successfully calculated and stored in '{age_column}'.")
        except Exception as e:
            print(f"Error calculating age from column '{birth_date_column}': {e}")
    else:
        print(f"Column '{birth_date_column}' not found in DataFrame.")

    return df


df = convert_columns_to_datetime(df, columns=config["etl"]["date_columns"])
df = convert_birth_year_to_datetime(df, column_name="birth_year")
df = calculate_age(df, birth_date_column="birth_year", age_column="age")
print(f"Columns: {df.columns}")
print(f"DTypes: {df.dtypes}")
# print(f"Samle data: {data.head}")
print(f"Shape of df: {df.shape}")


def convert_int_to_float(df, exclude_columns=None):
    """
    Converts integer columns in a DataFrame to float, excluding specified columns.

    Args:
        df (pd.DataFrame): The input DataFrame.
        exclude_columns (list, optional): List of column names to exclude from conversion.
                                          Default is None, meaning no exclusions.

    Returns:
        pd.DataFrame: DataFrame with integer columns converted to float, excluding specified columns.
    """
    if exclude_columns is None:
        exclude_columns = []

    # Automatically detect integer columns, excluding specified ones
    int_columns = [
        col
        for col in df.select_dtypes(include=["int64", "int32"]).columns
        if col not in exclude_columns
    ]

    for col in int_columns:
        df[col] = df[col].astype(float)
        print(f"Column '{col}' converted to float.")

    return df


df = convert_int_to_float(df, exclude_columns=config["etl"]["ids"])
print(f"Columns: {df.columns}")
print(f"DTypes: {df.dtypes}")
# print(f"Samle data: {data.head}")
print(f"Shape of df: {df.shape}")


def add_date_range_columns(df, date_column, id_column):
    """
    Adds four columns to the DataFrame:
    - 'start_date': Minimum date for each ID
    - 'end_date': Maximum date for each ID
    - 'max_date': The overall maximum date (constant for all rows)
    - 'min_date': The overall minimum date (constant for all rows)

    Args:
        df (pd.DataFrame): The input DataFrame.
        date_column (str): The name of the date column.
        id_column (str): The name of the ID column.

    Returns:
        pd.DataFrame: The updated DataFrame with four new columns.
    """
    if date_column not in df.columns or id_column not in df.columns:
        raise ValueError(
            f"Columns '{date_column}' or '{id_column}' not found in DataFrame."
        )

    # Convert to datetime if not already
    df[date_column] = pd.to_datetime(df[date_column], errors="coerce")

    # Compute start and end dates per ID
    df["start_date"] = df.groupby(id_column)[date_column].transform("min")
    df["end_date"] = df.groupby(id_column)[date_column].transform("max")

    # Compute global min/max dates
    min_date = df[date_column].min()
    max_date = df[date_column].max()

    df["min_date"] = min_date
    df["max_date"] = max_date

    return df


df = add_date_range_columns(df, date_column="date", id_column="player_key")
df.sort_values(by=["player_key", "date"])
print(f"Columns: {df.columns}")
print(f"DTypes: {df.dtypes}")
# print(f"Samle data: {data.head}")
print(f"Shape of df: {df.shape}")


# Calculate the duration of activity for each player
import pandas as pd


def calculate_duration(
    df, start_date_col="start_date", end_date_col="end_date", duration_col="duration"
):
    """
    Calculates the duration (difference in days) between end_date and start_date.

    Args:
        df (pd.DataFrame): The input DataFrame.
        start_date_col (str): The name of the column with the start date.
        end_date_col (str): The name of the column with the end date.
        duration_col (str): The name of the new column to store the duration.

    Returns:
        pd.DataFrame: The updated DataFrame with the 'duration' column added.
    """
    if start_date_col not in df.columns or end_date_col not in df.columns:
        raise ValueError(
            f"Columns '{start_date_col}' or '{end_date_col}' not found in DataFrame."
        )

    # Convert to datetime if not already
    df[start_date_col] = pd.to_datetime(df[start_date_col], errors="coerce")
    df[end_date_col] = pd.to_datetime(df[end_date_col], errors="coerce")

    # Calculate duration in days
    df[duration_col] = (df[end_date_col] - df[start_date_col]).dt.days

    return df


df = calculate_duration(df, start_date_col="start_date", end_date_col="end_date")
df.sort_values(by=["player_key", "date"])
print(f"Columns: {df.columns}")
print(f"DTypes: {df.dtypes}")
# print(f"Samle data: {data.head}")
print(f"Shape of df: {df.shape}")


# Calculate churn
import pandas as pd


def create_churn_column(
    df, end_date_col="end_date", max_date_col="max_date", churn_days=30
):
    """
    Creates a 'churn' column based on the difference between end_date and max_date.

    Args:
        df: Pandas DataFrame containing the end_date and max_date columns.
        end_date_col: Name of the column containing the end date (datetime).
        max_date_col: Name of the column containing the maximum date (datetime).
        churn_days: Number of days of inactivity to define churn.

    Returns:
        Pandas DataFrame with the added 'churn' column.
        Returns the original DataFrame if there are any errors during processing.
    """

    try:
        # Ensure date columns are datetime objects
        df[end_date_col] = pd.to_datetime(df[end_date_col])
        df[max_date_col] = pd.to_datetime(df[max_date_col])

        # Calculate the difference in days
        df["date_difference"] = (df[max_date_col] - df[end_date_col]).dt.days

        # Create the churn column
        df["churn"] = (df["date_difference"] > churn_days).astype(int)

        # Drop the temporary date_difference column (optional, but good practice)
        df = df.drop("date_difference", axis=1)

        return df

    except (KeyError, TypeError) as e:  # Handle potential errors gracefully
        print(f"Error creating churn column: {e}")
        return df  # Return original DataFrame if error occurs


df = create_churn_column(
    df, end_date_col="end_date", max_date_col="max_date", churn_days=30
)
print(f"Columns: {df.columns}")
print(f"DTypes: {df.dtypes}")
# print(f"Samle data: {data.head}")
print(f"Shape of df: {df.shape}")








# metrics to be used to create further features
metrics_cols = [
    "turnover_sum",
    "turnover_num",
    "NGR_sum",
    "deposit_sum",
    "deposit_num",
    "withdrawal_sum",
    "withdrawal_num",
    "login_num",
    "duration",
]





def add_row_number_within_player(df, player_id_col="player_key", date_col="date"):
    """Adds a row number within each player group, ordered by date.

    Args:
        df: Pandas DataFrame with player_id and date columns.
        player_id_col: Name of the player ID column.
        date_col: Name of the date column.

    Returns:
        Pandas DataFrame with the added 'row_number' column.
        Returns the original DataFrame if any error occurs.
    """
    try:
        df[date_col] = pd.to_datetime(df[date_col])  # Ensure date is datetime
        df = df.sort_values([player_id_col, date_col])  # Sort by player and date
        df = df.set_index(pd.RangeIndex(start=0, stop=len(df)))

        df["row_number"] = (
            df.groupby(player_id_col).cumcount() + 1
        )  # Efficiently add row numbers

        return df

    except (KeyError, TypeError) as e:
        print(f"Error adding row number: {e}")
        return df


df = add_row_number_within_player(df, player_id_col="player_key", date_col="date")
df.head()


df['player_key_backup'] = df['player_key']
df['player_key'] = df['player_key'].rank(method='dense').astype(int) #method='dense' is important as you want consecutive ranking
df.head()


def calculate_cumulative_metrics(
    df,
    key_col="player_key",
    date_col="date",
    start_date_col="start_date",
    end_date_col="end_date",
    metrics=None,
):
    """Calculates cumulative sums of metrics from start_date to each date for each player.

    Args:
        df: Pandas DataFrame with player_key, date, start_date, end_date and metric columns.
        player_key_col: Name of the player key column.
        date_col: Name of the date column.
        start_date_col: Name of the column with the start date.
        end_date_col: Name of the column with the end date.
        metrics: List of metric column names to calculate cumulative sums for.
                 If None, defaults to all numeric columns except player_key, date, start_date and end_date.

    Returns:
        Pandas DataFrame with added cumulative metric columns.
        Returns the original DataFrame if any error occurs.
    """

    try:
        df = df.sort_values([key_col, date_col])
        for metric in metrics:
            df[f'cumulative_{metric}'] = df.groupby(key_col)[metric].cumsum()

        return df  # Do NOT drop start_date and end_date

    except (KeyError, TypeError) as e:
        print(f"Error creating cumulative features: {e}")
        return df


metrics_cols = [
    "turnover_sum",
    "turnover_num",
    "NGR_sum",
    "deposit_sum",
    "deposit_num",
    "withdrawal_sum",
    "withdrawal_num",
    "login_num",
    "duration",
]
df = calculate_cumulative_metrics(
    df, key_col="player_key", date_col="date", metrics=metrics_cols
)


print(f"Columns: {df_cumulative.columns}")
print(f"DTypes: {df_cumulative.dtypes}")
print(f"Shape of df: {df_cumulative.shape}")
df.sort_values(by=["player_key", "date"]).head(10)


# calculate teh averageof teh metrics per day at each time point
def compute_per_day_metrics(df: pd.DataFrame) -> pd.DataFrame:
    """
    Computes per-day metrics by dividing cumulative columns by cumulative_duration.
    
    Args:
        df (pd.DataFrame): Input DataFrame containing cumulative metrics.
    
    Returns:
        pd.DataFrame: DataFrame with new per-day metric columns.
    """
    # Identify cumulative columns excluding "cumulative_duration"
    cumulative_columns = [col for col in df.columns if "cumulative_" in col and col != "cumulative_duration"]

    # Compute per-day metrics and rename columns
    for col in cumulative_columns:
        new_col_name = col.replace("cumulative_", "") + "_per_day"
        df[new_col_name] = (df[col] / df["cumulative_duration"]).round(4)

    return df



df = compute_per_day_metrics(df)
df.head(10)


# calculate the time between teh datapoints for each user
def compute_time_between_bets(df, key_id="player_key", date="date"):
    """
    Computes the time difference (in minutes) between consecutive bets for each player.
    If a player has no previous bet, assigns -1.

    Args:
        df (pd.DataFrame): Input DataFrame with 'player_key' and 'date'.

    Returns:
        pd.DataFrame: DataFrame with a new column 'time_between_bets'.
    """
    # Ensure 'date' is in datetime format
    df['date'] = pd.to_datetime(df['date'])

    # Sort values by player and date
    df = df.sort_values(by=['player_key', 'date'])

    # Compute time difference in minutes
    df['time_between_bets'] = df.groupby('player_key')['date'].diff().dt.days

    # Fill NaN (first bet) with -1
    df['time_between_bets'] = df['time_between_bets'].fillna(-1).astype(int)

    return df


df = compute_time_between_bets(df, key_id="player_key", date="date")
df.head(10)


# craete date features
def create_date_features(df: pd.DataFrame, date_column: str) -> pd.DataFrame:
    """
    Extracts day of the week and month from a date column, and applies one-hot encoding.

    Args:
        df (pd.DataFrame): Input DataFrame with a date column.
        date_column (str): The name of the date column.

    Returns:
        pd.DataFrame: DataFrame with new one-hot encoded features.
    """
    # Ensure the date column is in datetime format
    df[date_column] = pd.to_datetime(df[date_column])

    # Extract day of the week and month as categorical
    df['day_of_week'] = df[date_column].dt.strftime('%A')  # Monday, Tuesday, etc.
    df['month'] = df[date_column].dt.strftime('%B')  # January, February, etc.
    df['year'] = df[date_column].dt.year.astype(str)  # Convert year to string for encoding

    # One-hot encode day_of_week, month, and year
    df = pd.get_dummies(df, columns=['day_of_week', 'month', 'year'], prefix=['day', 'month', 'year'])
    # Convert only dummy variables to float (excluding the date column)
    dummy_columns = [col for col in df.columns if col.startswith(('day_', 'month_', 'year_'))]
    df[dummy_columns] = df[dummy_columns].astype(float)

    return df


df = create_date_features(df, date_column="date")
df.head()





import pandas as pd

def days_since_last_positive_metrics(df: pd.DataFrame, date_col: str, player_col: str, metrics: list) -> pd.DataFrame:
    """
    Computes the number of days since the last positive value of each metric for each player.
    This function calculates it for all data points per player.

    Args:
        df (pd.DataFrame): Input DataFrame with time series data.
        date_col (str): The name of the datetime column.
        player_col (str): The column representing unique players.
        metrics (list): List of metric columns to check for positive values.

    Returns:
        pd.DataFrame: DataFrame with new columns 'days_since_last_positive_<metric>' for each metric.
    """
    # Ensure the date column is in datetime format
    df[date_col] = pd.to_datetime(df[date_col])

    # Sort by player and date to ensure correct calculations
    df = df.sort_values(by=[player_col, date_col])

    # Initialize new columns for each metric
    for metric in metrics:
        df[f'days_since_last_positive_{metric}'] = -1  # Default value

        # Iterate over each player separately
        for player in df[player_col].unique():
            player_data = df[df[player_col] == player]
            
            # Track last positive date for the metric
            last_positive_date = None

            for idx, row in player_data.iterrows():
                if row[metric] > 0:
                    last_positive_date = row[date_col]  # Update last positive date
                elif last_positive_date is not None:
                    df.at[idx, f'days_since_last_positive_{metric}'] = (row[date_col] - last_positive_date).days

    return df




metrics_cols = [
    "turnover_sum",
    "turnover_num",
    "NGR_sum",
    "deposit_sum",
    "deposit_num",
    "withdrawal_sum",
    "withdrawal_num",
    "login_num",
]
df = days_since_last_positive_metrics(df, 'date', 'player_key', metrics_cols)



df = days_since_last_positive_metric





def create_metric_ratios(df: pd.DataFrame, ratio_pairs: list) -> pd.DataFrame:
    """
    Computes ratio features for given metric pairs.

    Args:
        df (pd.DataFrame): The input DataFrame with metric data.
        ratio_pairs (list): A list of tuples specifying (numerator, denominator) metric pairs.

    Returns:
        pd.DataFrame: DataFrame with new ratio columns.
    """
    for num, denom in ratio_pairs:
        ratio_col = f'ratio_{num}_to_{denom}'
        df[ratio_col] = df[num] / df[denom]

        # Handle division by zero or missing values
        df[ratio_col] = df[ratio_col].replace([float('inf'), -float('inf')], None).fillna(0)

    return df


metric_ratios = [
    ('turnover_sum', 'turnover_num'),
    ('NGR_sum', 'turnover_sum'),
    ('deposit_sum', 'withdrawal_sum')
]

df = create_metric_ratios(df, metric_ratios)
df.head()






def min_max_until_date(df: pd.DataFrame, date_col: str, player_col: str, metrics: list) -> pd.DataFrame:
    """
    Computes the minimum and maximum values of each metric until that date per player.

    Args:
        df (pd.DataFrame): Input DataFrame with time series data.
        date_col (str): The name of the datetime column.
        player_col (str): The column representing unique players.
        metrics (list): List of metric columns.

    Returns:
        pd.DataFrame: DataFrame with new columns for min and max values until that date.
    """
    # Ensure the date column is in datetime format
    df[date_col] = pd.to_datetime(df[date_col])

    # Sort by player and date to ensure correct calculations
    df = df.sort_values(by=[player_col, date_col])

    # Compute rolling min and max for each metric
    for metric in metrics:
        df[f'min_until_{metric}'] = df.groupby(player_col)[metric].expanding().min().reset_index(level=0, drop=True)
        df[f'max_until_{metric}'] = df.groupby(player_col)[metric].expanding().max().reset_index(level=0, drop=True)

    return df


metrics_cols = [
    "turnover_sum",
    "turnover_num",
    "NGR_sum",
    "deposit_sum",
    "deposit_num",
    "withdrawal_sum",
    "withdrawal_num",
    "login_num",
]
df = min_max_until_date(df, 'date', 'player_key', metrics_cols)
df.head()








itables.show(df)





# Generate EDA Report
profile_al = ProfileReport(
    df, title="Churn Data EDA", explorative=True, samples=None
)  # samples=None for all rows

# Display the report in Jupyter
profile_al.to_notebook_iframe()





def identify_binary_variables(df):
    """
    Identifies binary variables in a DataFrame.

    Args:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        list: A list of binary variable column names.
    """
    binary_columns = []

    for col in df.columns:
        unique_values = df[col].dropna().unique()
        if len(unique_values) == 2:
            binary_columns.append(col)

    print(f"Binary Variables Detected: {binary_columns}")
    return binary_columns


# Example: Assuming your DataFrame is loaded as 'df'
# df = pd.read_csv('your_data.csv')

# Identify binary variables
binary_vars = identify_binary_variables(df)





import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns


def eda_on_date_column(df, date_column):
    """
    Perform EDA on a date column: min, max, and frequency by month.

    Args:
        df (pd.DataFrame): The input DataFrame.
        date_column (str): The name of the date column.

    Returns:
        None
    """
    # Ensure the date column is in datetime format
    df[date_column] = pd.to_datetime(df[date_column], errors="coerce")

    # Drop rows where date conversion failed
    df = df.dropna(subset=[date_column])

    # Get the minimum and maximum dates
    min_date = df[date_column].min()
    max_date = df[date_column].max()

    print(f"Minimum Date in '{date_column}': {min_date}")
    print(f"Maximum Date in '{date_column}': {max_date}")

    # Frequency of records per month
    monthly_freq = df[date_column].dt.to_period("M").value_counts().sort_index()

    print("\nFrequency of Records per Month:")
    print(monthly_freq)

    # Plotting frequency per month
    plt.figure(figsize=(12, 6))
    monthly_freq.plot(kind="bar", color="skyblue")
    plt.title(f"Frequency of Records per Month ({date_column})")
    plt.xlabel("Month")
    plt.ylabel("Number of Records")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()


eda_on_date_column(df, "date")





import numpy as np
import pandas as pd
from scipy import stats


def detect_outliers_iqr(df, columns):
    """
    Detects outliers in specified columns of a DataFrame using the IQR method.

    Args:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of columns to check for outliers.

    Returns:
        dict: A dictionary with column names as keys and lists of outlier indices as values.
    """
    outliers = {}

    for col in columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1

            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outlier_indices = df[
                (df[col] < lower_bound) | (df[col] > upper_bound)
            ].index.tolist()
            outliers[col] = outlier_indices

            print(f"Outliers detected in '{col}': {len(outlier_indices)}")

    return outliers


def detect_outliers_zscore(df, columns, threshold=3):
    """
    Detects outliers in specified columns of a DataFrame using the Z-score method.

    Args:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of columns to check for outliers.
        threshold (float): Z-score threshold to identify outliers (default = 3).

    Returns:
        dict: A dictionary with column names as keys and lists of outlier indices as values.
    """
    outliers = {}

    for col in columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            z_scores = np.abs(stats.zscore(df[col].dropna()))
            outlier_indices = df[(z_scores > threshold)].index.tolist()
            outliers[col] = outlier_indices

            print(f"Outliers detected in '{col}': {len(outlier_indices)}")

    return outliers


# Define the columns for outlier detection
columns_to_check = [
    "turnover_sum",
    "turnover_num",
    "NGR_sum",
    "deposit_sum",
    "deposit_num",
    "withdrawal_sum",
    "withdrawal_num",
    "login_num",
]

# Detect outliers using IQR
print("Detect outliers using IQR:")
iqr_outliers = detect_outliers_iqr(df, columns_to_check)

# Detect outliers using Z-score
print("\nDetect outliers using Z-score:")

zscore_outliers = detect_outliers_zscore(df, columns_to_check)


# Visualize Outliers: Use boxplots for quick visualization:

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.boxplot(data=df[columns_to_check])
plt.xticks(rotation=45)
plt.title("Boxplot of Financial and Behavioral Features")
plt.show()


import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats


# Function to detect outliers using IQR and plot
def detect_outliers_iqr(df, columns):
    """
    Detects outliers in specified columns of a DataFrame using the IQR method and plots them in red.

    Args:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of columns to check for outliers.

    Returns:
        dict: A dictionary with column names as keys and lists of outlier indices as values.
    """
    outliers = {}

    for col in columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1

            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outlier_indices = df[
                (df[col] < lower_bound) | (df[col] > upper_bound)
            ].index.tolist()
            outliers[col] = outlier_indices

            print(f"Outliers detected in '{col}': {len(outlier_indices)}")

            # Plotting with outliers highlighted in red
            plt.figure(figsize=(8, 5))
            sns.boxplot(x=df[col], color="skyblue", width=0.5)

            # Highlight outliers in red
            outlier_values = df.loc[outlier_indices, col]
            plt.scatter(
                outlier_values,
                [0] * len(outlier_values),
                color="red",
                s=100,
                label="Outlier",
            )

            plt.title(f"{col} with Outliers Highlighted (IQR)")
            plt.xlabel(col)
            plt.legend(["Outlier"], loc="upper right")
            plt.show()

    return outliers


# Function to detect outliers using Z-score and plot
def detect_outliers_zscore(df, columns, threshold=3):
    """
    Detects outliers in specified columns of a DataFrame using the Z-score method and plots them in red.

    Args:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of columns to check for outliers.
        threshold (float): Z-score threshold to identify outliers (default = 3).

    Returns:
        dict: A dictionary with column names as keys and lists of outlier indices as values.
    """
    outliers = {}

    for col in columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            z_scores = np.abs(stats.zscore(df[col].dropna()))
            outlier_indices = df[z_scores > threshold].index.tolist()
            outliers[col] = outlier_indices

            print(f"Outliers detected in '{col}': {len(outlier_indices)}")

            # Plotting with outliers highlighted in red
            plt.figure(figsize=(8, 5))
            sns.boxplot(x=df[col], color="lightgreen", width=0.5)

            # Highlight outliers in red
            outlier_values = df.loc[outlier_indices, col]
            plt.scatter(
                outlier_values,
                [0] * len(outlier_values),
                color="red",
                s=100,
                label="Outlier",
            )

            plt.title(f"{col} with Outliers Highlighted (Z-Score)")
            plt.xlabel(col)
            plt.legend(["Outlier"], loc="upper right")
            plt.show()

    return outliers


# Define the columns for outlier detection
columns_to_check = [
    "turnover_sum",
    "turnover_num",
    "NGR_sum",
    "deposit_sum",
    "deposit_num",
    "withdrawal_sum",
    "withdrawal_num",
    "login_num",
]

# Detect outliers using IQR
print("Detect outliers using IQR:")
iqr_outliers = detect_outliers_iqr(df, columns_to_check)

# Detect outliers using Z-score
print("\nDetect outliers using Z-score:")

zscore_outliers = detect_outliers_zscore(df, columns_to_check)





def count_id_occurrences(df, id_column, count_column_name="id_count"):
    """
    Adds a new column that counts the number of times each ID appears in the dataset.

    Args:
        df (pd.DataFrame): The input DataFrame.
        id_column (str): The name of the ID column.
        count_column_name (str): The name of the new column with count values (default: 'id_count').

    Returns:
        pd.DataFrame: The DataFrame with the new count column added.
    """
    if id_column not in df.columns:
        raise ValueError(f"Column '{id_column}' not found in DataFrame.")

    # Count occurrences of each ID
    id_counts = df[id_column].value_counts().to_dict()

    # Map counts back to the DataFrame
    df[count_column_name] = df[id_column].map(id_counts)

    return df


df = count_id_occurrences(data, id_column="player_key")


df









def keep_numeric_and_convert_to_float(df: pd.DataFrame) -> pd.DataFrame:
    """
    Removes all columns that are dates and strings, keeping only numeric columns.
    Converts all integer columns to float.

    Args:
        df (pd.DataFrame): Input DataFrame.

    Returns:
        pd.DataFrame: DataFrame with only numeric columns (floats).
    """
    # Select only numeric columns (float, int)
    numeric_df = df.select_dtypes(include=['number']).copy()

    # Convert integer columns to float
    for col in numeric_df.columns:
        if pd.api.types.is_integer_dtype(numeric_df[col]):
            numeric_df[col] = numeric_df[col].astype(float)

    return numeric_df


df_numeric = keep_numeric_and_convert_to_float(df)
df_numeric = df_numeric.drop(columns=["player_key_small","player_key_backup"], errors='ignore')
df_numeric.head()
df_numeric.columns
df_numeric.player_key.head(10)


# quality checks
import numpy as np

# Find columns with NaN values
nan_columns = X.columns[X.isna().any()].tolist()
print("Columns containing NaN values:", nan_columns)

# Find columns with Inf values
inf_columns = X.columns[np.isinf(X).any()].tolist()
print("Columns containing Inf values:", inf_columns)
# Lists of problematic columns
nan_columns = ['metric_per_day', 'turnover_sum_per_day', 'turnover_num_per_day', 
               'NGR_sum_per_day', 'deposit_sum_per_day', 'deposit_num_per_day', 
               'withdrawal_sum_per_day', 'withdrawal_num_per_day']

inf_columns = ['metric_per_day', 'turnover_sum_per_day', 'turnover_num_per_day', 
               'NGR_sum_per_day', 'deposit_sum_per_day', 'deposit_num_per_day', 
               'withdrawal_sum_per_day', 'withdrawal_num_per_day', 'login_num_per_day']

# drop bad columns
df_numeric = df_numeric.drop(columns=inf_columns, errors='ignore')





import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    confusion_matrix, roc_curve, auc, precision_recall_curve, 
    accuracy_score, precision_score, recall_score, f1_score
)
from sklearn.utils import shuffle

def time_series_cv(df, date_col, player_col, n_splits=3):
    """
    Performs time-aware cross-validation for churn prediction.
    
    Args:
        df (pd.DataFrame): Input DataFrame with time-ordered player data.
        date_col (str): Column representing time (row_number or date).
        player_col (str): Column representing unique players.
        n_splits (int): Number of CV folds.
    
    Returns:
        List of train-test index splits.
    """
    df = df.sort_values(by=[player_col, date_col])  # Ensure time ordering

    tscv = TimeSeriesSplit(n_splits=n_splits)
    features = df.drop(columns=[player_col, 'churn'])

    return [(train_idx, test_idx) for train_idx, test_idx in tscv.split(features)]


# Train-Test Split (Time-Based)
cv_splits = time_series_cv(df_numeric, 'row_number', 'player_key', n_splits=3)

# Features & Target
X = df_numeric.drop(columns=['player_key', 'churn'])  # Features
y = df_numeric['churn']  # Target
# Define Random Forest Model
rf_model = RandomForestClassifier(random_state=42)

# Define Hyperparameter Grid
param_grid = {
    'n_estimators': [50],
    'max_depth': [5, 10],
    # 'min_samples_split': [2, 5, 10],
    # 'min_samples_leaf': [1, 2, 5]
}

# Run GridSearchCV with Time Series CV
grid_search = GridSearchCV(
    rf_model, 
    param_grid, 
    cv=cv_splits, 
    scoring='f1', 
    n_jobs=2, 
    verbose=1
)
grid_search.fit(X, y)
# grid_search.fit(X, y)



import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    confusion_matrix, roc_curve, auc, precision_recall_curve, 
    accuracy_score, precision_score, recall_score, f1_score
)
from sklearn.utils import shuffle

def time_series_cv(df, date_col, player_col, n_splits=3):
    """
    Performs time-aware cross-validation for churn prediction.
    
    Args:
        df (pd.DataFrame): Input DataFrame with time-ordered player data.
        date_col (str): Column representing time (row_number or date).
        player_col (str): Column representing unique players.
        n_splits (int): Number of CV folds.
    
    Returns:
        List of train-test index splits.
    """
    df = df.sort_values(by=[player_col, date_col])  # Ensure time ordering

    tscv = TimeSeriesSplit(n_splits=n_splits)
    features = df.drop(columns=[player_col, 'churn'])

    return [(train_idx, test_idx) for train_idx, test_idx in tscv.split(features)]


# Train-Test Split (Time-Based)
cv_splits = time_series_cv(df_numeric, 'row_number', 'player_key', n_splits=3)

# Features & Target
X = df_numeric.drop(columns=['player_key', 'churn'])  # Features
y = df_numeric['churn']  # Target

# Define Random Forest Model
rf_model = RandomForestClassifier(random_state=42)

# Define Hyperparameter Grid
param_grid = {
    'n_estimators': [10, 20, 30],
    'max_depth': [5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5]
}

# Run GridSearchCV with Time Series CV
grid_search = GridSearchCV(
    rf_model, 
    param_grid, 
    cv=cv_splits, 
    scoring='f1', 
    n_jobs=2, 
    verbose=1
)


grid_search.fit(X, y)
best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# ========== 4️⃣ Evaluate Model ==========
y_pred = best_rf.predict(X)
y_pred_proba = best_rf.predict_proba(X)[:, 1]

accuracy = accuracy_score(y, y_pred)
precision = precision_score(y, y_pred)
recall = recall_score(y, y_pred)
f1 = f1_score(y, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# ========== 5️⃣ Generate Performance Plots ==========

# 5.1 Confusion Matrix
def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

plot_confusion_matrix(y, y_pred)

# 5.2 ROC-AUC Curve
def plot_roc_curve(y_true, y_proba):
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC-AUC Curve")
    plt.legend()
    plt.show()

plot_roc_curve(y, y_pred_proba)

# 5.3 Precision-Recall Curve
def plot_precision_recall_curve(y_true, y_proba):
    precision, recall, _ = precision_recall_curve(y_true, y_proba)
    plt.figure(figsize=(6, 5))
    plt.plot(recall, precision, label="Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.legend()
    plt.show()

plot_precision_recall_curve(y, y_pred_proba)

# 5.4 Feature Importance
def plot_feature_importance(model, feature_names):
    importance = model.feature_importances_
    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
    
    plt.figure(figsize=(8, 5))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
    plt.title("Feature Importance")
    plt.xlabel("Importance Score")
    plt.ylabel("Feature")
    plt.show()

plot_feature_importance(best_rf, X.columns)

# 5.5 Learning Curve
from sklearn.model_selection import learning_curve

def plot_learning_curve(model, X, y):
    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=3, scoring='f1', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))
    train_mean, train_std = np.mean(train_scores, axis=1), np.std(train_scores, axis=1)
    test_mean, test_std = np.mean(test_scores, axis=1), np.std(test_scores, axis=1)

    plt.figure(figsize=(8, 5))
    plt.plot(train_sizes, train_mean, label="Training Score", color='blue')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)
    plt.plot(train_sizes, test_mean, label="Cross-Validation Score", color='red')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='red', alpha=0.2)
    
    plt.xlabel("Training Sample Size")
    plt.ylabel("F1 Score")
    plt.title("Learning Curve")
    plt.legend()
    plt.show()

plot_learning_curve(best_rf, X, y)






import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    confusion_matrix, roc_curve, auc, precision_recall_curve, 
    accuracy_score, precision_score, recall_score, f1_score
)
from sklearn.utils import shuffle

def time_series_cv(df, date_col, player_col, n_splits=3):
    """
    Performs time-aware cross-validation for churn prediction.
    
    Args:
        df (pd.DataFrame): Input DataFrame with time-ordered player data.
        date_col (str): Column representing time (row_number or date).
        player_col (str): Column representing unique players.
        n_splits (int): Number of CV folds.
    
    Returns:
        List of train-test index splits.
    """
    df = df.sort_values(by=[player_col, date_col])  # Ensure time ordering

    tscv = TimeSeriesSplit(n_splits=n_splits)
    features = df.drop(columns=[player_col, 'churn'])

    return [(train_idx, test_idx) for train_idx, test_idx in tscv.split(features)]


# Train-Test Split (Time-Based)
cv_splits = time_series_cv(df, 'row_number', 'player_key', n_splits=3)

# Features & Target
X = df.drop(columns=['player_key', 'churn'])  # Features
y = df['churn']  # Target

# Define Random Forest Model
rf_model = RandomForestClassifier(random_state=42)

# Define Hyperparameter Grid
param_grid = {
    'n_estimators': [50],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Run GridSearchCV with Time Series CV
grid_search = GridSearchCV(
    rf_model, 
    param_grid, 
    cv=cv_splits, 
    scoring='f1', 
    n_jobs=2, 
    verbose=1
)


from dask.distributed import Client
import joblib

client = Client()  # Start Dask Client

with joblib.parallel_backend('dask'):
    grid_search.fit(X, y)  # Run GridSearchCV using Dask backend

best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# ========== 4️⃣ Evaluate Model ==========
y_pred = best_rf.predict(X)
y_pred_proba = best_rf.predict_proba(X)[:, 1]

accuracy = accuracy_score(y, y_pred)
precision = precision_score(y, y_pred)
recall = recall_score(y, y_pred)
f1 = f1_score(y, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# ========== 5️⃣ Generate Performance Plots ==========

# 5.1 Confusion Matrix
def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

plot_confusion_matrix(y, y_pred)

# 5.2 ROC-AUC Curve
def plot_roc_curve(y_true, y_proba):
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC-AUC Curve")
    plt.legend()
    plt.show()

plot_roc_curve(y, y_pred_proba)

# 5.3 Precision-Recall Curve
def plot_precision_recall_curve(y_true, y_proba):
    precision, recall, _ = precision_recall_curve(y_true, y_proba)
    plt.figure(figsize=(6, 5))
    plt.plot(recall, precision, label="Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.legend()
    plt.show()

plot_precision_recall_curve(y, y_pred_proba)

# 5.4 Feature Importance
def plot_feature_importance(model, feature_names):
    importance = model.feature_importances_
    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
    
    plt.figure(figsize=(8, 5))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
    plt.title("Feature Importance")
    plt.xlabel("Importance Score")
    plt.ylabel("Feature")
    plt.show()

plot_feature_importance(best_rf, X.columns)

# 5.5 Learning Curve
from sklearn.model_selection import learning_curve

def plot_learning_curve(model, X, y):
    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=3, scoring='f1', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))
    train_mean, train_std = np.mean(train_scores, axis=1), np.std(train_scores, axis=1)
    test_mean, test_std = np.mean(test_scores, axis=1), np.std(test_scores, axis=1)

    plt.figure(figsize=(8, 5))
    plt.plot(train_sizes, train_mean, label="Training Score", color='blue')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)
    plt.plot(train_sizes, test_mean, label="Cross-Validation Score", color='red')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='red', alpha=0.2)
    
    plt.xlabel("Training Sample Size")
    plt.ylabel("F1 Score")
    plt.title("Learning Curve")
    plt.legend()
    plt.show()

plot_learning_curve(best_rf, X, y)










